\section{Poisson Distribution}
\label{sec:poisson-distribution}

\opt{solutionfiles}{
\subsection*{Theory and Exercises}
\Opensolutionfile{hint}
\Opensolutionfile{ans}
}

In this section, we provide motivation for the use of the Poisson process as an arrival process of customers or jobs at a shop, service station, or machine to receive service. In the exercises we derive numerous properties of this exceedingly important distribution; in the rest of the book we will use these results time and again.

Consider a stream of customers that enter a shop over time.
Let us write $N(t)$ for the number of customers that enters during the time interval $[0,t]$ and $N(s, t] = N(t)-N(s)$ for the number that arrive in the time period $(s, t]$.
Clearly, as we do not know in advance how many customers will enter, we model the set $\{N(t), t\geq 0\}$ as a family of random variables.

Our first assumption is that the rate at which customers enter stays constant over time. Then it is reasonable to assume that
the expected number of arrivals is proportional to the length of
the interval. Hence, it is reasonable to assume that there exists some
constant~$\lambda$ such that
\begin{equation}
 \label{eq:6}
 \E{N(s,t]} = \lambda (t-s).
\end{equation}
The constant $\lambda$ is called the \recall{arrival rate} of the arrival process.


The second assumption is that the process $N_\lambda = \{N(t), t\geq 0\}$ has \recall{stationary and independent increments}.
Stationarity means that the distributions of the number of arrivals are the same for all intervals of equal length, that is, 
$N(s,t]$ has the same distribution as $N(u, v]$ if $t-s = v-u$.
Independence means, roughly speaking, that knowing that $N(s,t]= n$, does not help to make any predictions about the value of $N(u, v]$ if the intervals $(s,t]$ and $(u, v]$ do not overlap.



To find the distribution of $N(t)$ for some given $t$, let us split the interval $[0,t]$ into $n$ sub-intervals, all of equal length, and ask: `What is the probability that a customer arrives in some given sub-interval?'
By our first assumption, the arrival rate is constant over time.
Therefore, the probability~$p$ of an arrival in each interval should be constant.
Moreover, if the time intervals are very small, we can safely neglect the probability that two or more customers arrive in one interval.

As a consequence, then, we can model the occurrence of an arrival in some period $i$ as a Bernoulli distributed random variable $B_i$ such that $p=\P{B_i =1}$ and $\P{B_i=0} = 1-\P{B_i = 1}$, and we assume that $B_i$ and $B_j$ are independent whenever $i\neq j$.
The total number of arrivals $N_n(t)$ that occur in $n$ intervals is then \recall{binomially distributed}, i.e.,
\begin{equation}\label{eq:bin}
 \P{N_n(t) = k} = {n \choose k} p^k (1-p)^{n-k}.
\end{equation}

If we take $n\to\infty$, $p\to0$ such that $n p=\lambda t$, then $N_n(t)$ converges (in distribution) to a \recall{Poisson distributed} random variable $N(t)$, i.e., 
\begin{equation}\label{eq:pois}
 \P{N(t) = k} = 
e^{-\lambda t} \frac{(\lambda t)^k}{k!}, 
\end{equation}
and then we write $N(t)\sim P(\lambda t)$.


\begin{extra}
Show that $\E{N_n(t)} = \sum_{i=1}^n \E{B_i} = n p$. Conclude that we need to choose $p = \lambda t/n$ if we want that $\E{N_n(t)} = \E{N(t)}$.
\begin{hint}
Use that $\E{X+Y} = \E X + \E Y$. 
\end{hint}
\begin{solution}
 \begin{equation*}
 \E{N_n(t)} = \E{\sum_{i=1}^n {B_i}} = \sum_{i=1}^n \E{B_i} = n \E{B_i} = n p.
 \end{equation*}
 By~\cref{eq:6} $\E{N(t)} = \lambda t$. Now we want the expectations of $N_n(t)$ and $N(t)$ to be the same, thus, $n p = \lambda t$. 
\end{solution}
\end{extra}

\begin{extra}
What is the difference between $N_n(t)$ and $N(t)$?
\begin{solution}
 $N_n(t)$ is a binomially distributed random variable with parameters $n$ and $p$.
 The maximum value of $N_n(t)$ is $n$.
 The random variable $N(t)$ models the number of arrivals that can occur during $[0,t]$.
 As such it is not necessarily bounded by $n$.
 Thus, $N_n(t)$ and $N(t)$ cannot represent the same random variable.
\end{solution}
\end{extra}

\begin{extra} \clabel{ex:31}
 Show that the binomial distribution in~\cref{eq:bin} converges to the Poisson distribution, i.e., 
 \begin{equation*}\label{eq:52}
 \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!},
 \end{equation*}
 if $n\to\infty$, $p\to0$ such that $n p=\lambda t$.
\begin{hint}
 First find $p$, $n$, $\lambda$ and $t$ such that the rate at which an event occurs in both processes are the same.
 Then consider the binomial distribution and use the standard limit $(1-x/n)^n \to e^{-x}$ as $n\to \infty$.
\end{hint}
\begin{solution}
% Substituting this in the expression for $\P{N_n(t)=k}$ gives
% \begin{equation*}
% \P{N_n(t) = k} = {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k}.
% \end{equation*}
% To see that 
% \begin{equation*}\label{eq:52}
% \lim_{n\to\infty} {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} = e^{-\lambda t} \frac{(\lambda t)^k}{k!},
% \end{equation*}
% use that
 \begin{align*}
 {n \choose k} \left(\frac{\lambda t}{n}\right)^k \left(1-\frac{\lambda t}n\right)^{n-k} 
&= \frac{n!}{k!(n-k)!} \left(\frac{\lambda t}{n}\frac{n}{n-\lambda t}\right)^k \left(1-\frac{\lambda t}n\right)^{n} \\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n!}{n^k(n-k)!}\left(1-\frac{\lambda t}n\right)^{n}\\
&= \frac{(\lambda t)^k}{k!} \left(\frac n{n-\lambda t} \right)^k \frac{n}{n}\frac{n-1}{n}\cdots\frac{n-k+1}{n} \left(1-\frac{\lambda t}n\right)^{n}.
\end{align*}
Observe now that, as $\lambda t$ is finite, $n/(n-\lambda t)\to 1$ as
$n\to \infty$. Also for any finite $k$, $(n-k)/n\to1$. Finally, use~\cref{eq:65} to see that
$\left(1-\frac{\lambda t}n\right)^{n} \to e^{-\lambda t}$.
% The rest
% is easy, so that, as $n\to\infty$, the above converges to 
% \begin{equation*}
% \frac{(\lambda t)^k}{k!} e^{-\lambda t}.
% \end{equation*}

\end{solution}
\end{extra}



We call the process $N_\lambda=\{N(t)\}$ a \recall{Poisson process} with rate $\lambda$ when $N_\lambda$ is stationary, has independent increments, and its elements $N(t)\sim P(\lambda t)$ for all~$t$.
Observe that the process $N_\lambda$ is a much more complicated object than a Poisson distributed random variable.
The process is an uncountable set of random variables indexed by $t\in \R^+$, not just \emph{one} random variable.


In the remainder of this section we derive a number of properties of the Poisson process that we will use time and again.


\begin{exercise} \clabel{ex:35}
 Show that 
 \begin{equation*}
\P{N(t+h) = n \given N(t) = n} = 1-\lambda h + o(h)
 \end{equation*}
when $N(t) \sim P(\lambda t)$ and $h$ is small. 
\begin{hint}
Use the definition of the conditional probability and small $o$ notation.

% Think about the meaning of the formula $\P{N(t+h) = n \given N(t) = n}$.
% It is a conditional probability that should be read like this: given that up to time $t$ we have seen $n$ arrivals (i.e., $N(t)=n$), what is the probability that just a little later (at $t+h$) the number of arrivals is still $n$, i.e., $N(t+h)=n$?
% Then use the definition of the Poisson distribution to compute this probability.
\end{hint}
\begin{solution}
Write $N(s, t]$ for the number of arrivals in the interval $(s,t]$. First we make a few simple observations: $N(t+h]= N(t) + N(t, t+h]$, hence
\begin{equation*}
 \begin{split}
 \1{N(t+h)=n, N(t)=n}
&= \1{N(t) + N(t, t+h] = n, N(t)=n} = \\
&\1{N(t, t+h] = 0, N(t)=n}.
 \end{split}
\end{equation*}
Thus, 
 \begin{align*}
 \P{N(t+h) = n | N(t) = n} 
&= \frac{\P{N(t+h) = n, N(t) = n}}{\P{N(t)=n}} \\
&= \frac{\P{N(t, t+h] = 0, N(t) = n}}{\P{N(t)=n}} \\
&= \frac{\P{N(t, t+h] = 0} \P{N(t) = n}}{\P{N(t)=n}} \quad \text{(independence)}\\
%& \quad\text{(by independence of } N(t, t+h] \text{ and } N(t))\\
&= \P{N(t, t+h] = 0} \\
&= \P{N(0, h] = 0} \quad\text{(stationarity)} \\
&= e^{-\lambda h} (\lambda h)^0/0! \\
&= e^{-\lambda h} = 1-\lambda h + o(h).
 \end{align*}
\end{solution}
\end{exercise}

\begin{extra}
 Show that
 \begin{equation*}
\P{N(t+h) = n+1 \given N(t) = n} = \lambda h + o(h)
 \end{equation*}
when $N(t) \sim P(\lambda t)$ and $h$ is small. 
\begin{hint} Use ~\cref{ex:35}.
\end{hint}
\begin{solution}
 \begin{align*}
 \P{N(t+h) = n +1 | N(t) = n} 
&= \P{N(t+h) = n +1 , N(t) =n}/\P{N(t) = n}\\
&= \P{N(t, t+h] = 1} = e^{-\lambda h} (\lambda h)^1/1! \\
&= (1-\lambda h + o(h))\lambda h = \lambda h - \lambda^2 h^2 + o(h) \\
&= \lambda h + o(h). 
 \end{align*}
\end{solution}
\end{extra}

\begin{extra}
 Show that if $N(t) \sim P(\lambda t)$, we have for small $h$,
 \begin{equation*}
 \P{N(t+h) \geq n+2 \given N(t) = n} = o(h).
 \end{equation*}
\begin{hint}
 Use that $\sum_{i=2}^\infty x^i/i! = \sum_{i=0}^\infty x^i/i! - x -1 = e^x -x - 1$.
\end{hint}
\begin{solution}
 \begin{align*}
 \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} \\
&= e^{-\lambda h} \sum_{i=2}^\infty \frac{(\lambda h)^i}{i!} 
= e^{-\lambda h} \left(\sum_{i=0}^\infty \frac{(\lambda h)^i}{i!} - \lambda h - 1\right)\\
&= e^{-\lambda h}(e^{\lambda h} - 1 - \lambda h) 
= 1 - e^{-\lambda h}(1 + \lambda h) \\
&= 1 - (1-\lambda h + o(h))(1+\lambda h) 
= 1 - (1-\lambda^2 h^2 + o(h)) \\
&= \lambda^2 h^2 + o(h) = o(h).
 \end{align*}

We can also use the results of the previous parts to see that
\begin{align*}
 \P{N(t+h) \geq n+2 | N(t) = n} 
&= \P{N(t, t+h] \geq 2} = 1- \P{N(t, t+h]<2} \\
&= 1 - \P{N(t, t+h]= 0} - \P{N(t, t+h]=1} \\
&= 1 - (1-\lambda h + o(h) ) - (\lambda h + o(h)) \\
&= o(h).
\end{align*}
\end{solution}
\end{extra}


\begin{extra}
Show that for a Poisson process $N_\lambda$, 
\begin{equation*}
\P{N(s) =1\given N(t)=1} = \frac s t,
\end{equation*}
if $s\in[0,t]$. Thus, if you know that an arrival occurred during $[0,t]$, the arrival is distributed
uniformly on the interval $[0,t]$. Note that this probability is independent of $\lambda$. 
\begin{hint}
 Observe that 
 \begin{equation*}
%\{N(0,s]+N(s,t]=1\}\cap\{N(0,s]=1\} = \{1+N(s,t]=1\}\cap\{N(0,s]=1\}=\{N(s,t]=0\}\cap\{N(0,s]=1\}.
\1{N(0,s]+N(s,t]=1}\1{N(0,s]=1} = \1{1+N(s,t]=1}\1{N(0,s]=1}=\1{N(s,t]=0}\1{N(0,s]=1}.
 \end{equation*}
Use independence and~\cref{eq:74}.
\end{hint}
\begin{solution}
From the hint,
\begin{align*}
 \P{N(0,s] =1\given N(0,t]=1} 
&= \frac{\P{N(0,s] =1, N(0,t]=1}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1, N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\P{N(0,s] =1}\P{N(s,t]=0}}{\P{N(0,t]=1}} \\
&= \frac{\lambda s e^{-\lambda s} e^{-\lambda (t-s)}}{\lambda t e^{-\lambda t}} = \frac s t.
\end{align*}
\end{solution}
\end{extra}
 
\begin{exercise} \clabel{ex:2}
 Show that if $N(t)\sim P(\lambda t)$, the expected number of arrivals during $[0,t]$ is
 \begin{equation*}
 \E{N(t)} = \lambda t.
 \end{equation*}
\begin{hint}
Use~\cref{eq:66}. Note that the term with $n=0$ does not contribute in the following summation
\begin{equation*}
\sum_{n=0}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty n \frac{\lambda^n}{n!} = \sum_{n=1}^\infty \frac{\lambda^n}{(n-1)!} = \lambda \sum_{n=0}^\infty \frac{\lambda^n}{n!} = \lambda e^{\lambda},
\end{equation*}
where we apply a change of notation in the second to last step.
\end{hint}
\begin{solution} 
 When a random variable~$N$ is Poisson distributed with parameter
 $\lambda t$,
 \begin{align*}
 \E N 
&= \sum_{n=0}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!} \\
&= \sum_{n=1}^\infty n e^{-\lambda t}\frac{(\lambda t)^n}{n!} \\ 
&= e^{-\lambda t} \lambda t \sum_{n=1}^\infty \frac{(\lambda t)^{n-1}}{(n-1)!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty \frac{(\lambda t)^{n}}{n!} \\
&= e^{-\lambda t} \lambda t e^{\lambda t} \\
&= \lambda t.
 \end{align*}
\end{solution}
\end{exercise}


\begin{extra}
 Show that if $N(t)\sim P(\lambda t)$, the variance of the number of arrivals during $[0,t]$ is
 \begin{equation*}
\V{N(t)} = \lambda t.
 \end{equation*}
\begin{hint} Use~\cref{eq:68}. Compute $\E{N^2}$ and use~\cref{ex:2}.
\end{hint}
\begin{solution} 

 \begin{align*}
 \E{N^2}
&= \sum_{n=0}^\infty n^2 e^{-\lambda t}\frac{(\lambda t)^n}{n!} \\
&= e^{-\lambda t} \sum_{n=1}^\infty n \frac{(\lambda t)^n}{(n-1)!} \\
&= e^{-\lambda t} \sum_{n=0}^\infty (n+1) \frac{(\lambda t)^{n+1}}{n!} \\
&= e^{-\lambda t} \lambda t \sum_{n=0}^\infty n \frac{(\lambda t)^{n}}{n!} +e^{-\lambda t}\lambda t \sum_{n=0}^\infty\frac{(\lambda t)^{n}}{n!} \\
&\quad\text{now use the hint to simplify the first summation} \\
&= (\lambda t)^2 + \lambda t.
\end{align*}
Hence, $\V N = \E{N^2} - (\E N)^2 = (\lambda t)^2 + \lambda t - (\lambda t)^2 = \lambda t$.
\end{solution}
\end{extra}

\begin{extra} \clabel{ex:53}
Show that the moment-generating function of the random variable~$N(t)\sim P(\lambda t)$ is
\begin{equation*}
M_{N(t)}(s) 
%= \E{e^{sN(t)}} 
= \exp{(\lambda t(e^s-1))}.
\end{equation*}
\begin{hint}
Use~\cref{eq:66} with $f(x) = e^{sx}$. \end{hint}
\begin{solution}
Since $N(t)$ is Poisson distributed with parameter $\lambda t$, 
\begin{align*}
M_{N(t)}(s)
&= \E{e^{s N(t)}} \\
&= \sum_{k=0}^\infty e^{s k} \P{N(t)=k} \\
%&\quad\text{where we use that $\E{f(N(t))}=\sum_{n=0}^\infty f(n) \P{N(t)=n}$}\\
&= \sum_{k=0}^\infty e^{s k} \frac{(\lambda t)^k}{k!} e^{-\lambda t} \\
&= e^{-\lambda t} \sum_{k=0}^\infty \frac{(e^s \lambda t)^k}{k!} \\
&= \exp(-\lambda t + e^s \lambda t) =\exp(\lambda t(e^s - 1)).
\end{align*}
\end{solution}
\end{extra}

\begin{exercise} \clabel{ex:l-101}
 Use the moment-generating function of $N(t)\sim P(\lambda t)$ to compute $\E{N(t)}$ and $\V{N(t)}$. 
\begin{hint}
Use~\cref{eq:69} and~\cref{eq:64}. 
\end{hint}
\begin{solution}
Using the expression for the moment-generating function of~\cref{ex:53},
 \begin{equation*}
 M_{N(t)}'(s) = \lambda t e^s \exp(\lambda t(e^s - 1)).
 \end{equation*}
Hence $\E{N(t)} = M_{N(t)}'(0) = \lambda t $. Next, 
 \begin{equation*}
 M_{N(t)}''(s) = (\lambda t e^s + (\lambda t e^s)^2) \exp(\lambda t(e^s - 1)),
 \end{equation*}
hence $\E{(N(t))^2} = M''(0) = \lambda t + (\lambda t)^2$. And thus, 
\begin{equation*}
\V{N(t)} =\E{(N(t))^2}-(\E{N(t)})^2 = \lambda t + (\lambda t)^2 - (\lambda t)^2 = \lambda t.
\end{equation*}
\end{solution}
\end{exercise}

Define the \recall{square coefficient of variation} (\recall{SCV}) of a random variable~$X$ as 
\begin{equation}\label{eq:62}
 C^2= \frac{\V X}{(\E X)^2}.
\end{equation}
As will become clear later, the SCV is a very important concept in
 queueing theory. Memorize it as a measure of \emph{relative
 variability}.

\begin{exercise} \clabel{ex:l-102}
 Show that the SCV of $N(t)\sim P(\lambda t)$ is equal to $1/(\lambda t)$. What does this mean for $t$ large?
\begin{solution}
 \begin{equation*}
SCV = \frac{\V{N(t)}}{(\E{N(t)})^2} = \frac{\lambda t}{(\lambda t)^2} = \frac1{\lambda t}.
 \end{equation*}
The relative variability of the Poisson process goes down as $t\to\infty$. 
\end{solution}
\end{exercise}




\recall{Merging} Poisson processes occurs often in practice.
We have two Poisson processes, for instance, the arrival processes $N_\lambda$ of men and $N_\mu$ of women at a shop.
In the figure below, each cross represents an arrival; in the upper line it corresponds to a man, in the middle line to a woman, and in the lower line to an arrival of a general customer at the shop.
Thus, the shop `sees' the superposition of these two arrival processes.
In fact, this merged process $N_{\lambda+\mu}$ is also a Poisson process with rate $\lambda+\mu$.


 \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);

\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda(t)$};
\draw[->] (0,1)--(10,1);
\node[left] at (0,1) {$N_\mu(t)$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda+\mu}(t)$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (1.5,1.06)--(1.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.2,2.06)--(3.2,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (3.5,1.06)--(3.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (4.5,1.06)--(4.5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (5,1.06)--(5,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (6.1,1.06)--(6.1,-0.06);
\draw[{Rays[]}-{Rays[]},dotted] (7.1,2.06)--(7.1,-0.06);
\end{tikzpicture}
\end{center}



\begin{exercise}\clabel{ex:l-103} 
If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show with a conditioning argument that
$N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$. 
\begin{hint}
 Use sets $\{N_\lambda(t) = i\}$ to decompose $\{N_\lambda(t) + N_\mu(t) = n\}$. With this observe that
 \begin{equation*}
 \1{N_\lambda(t) + N_\mu(t) = n} = 
 \sum_{i=0}^n \1{N_\lambda(t)=i, N_\mu(t) = n-i}.
 \end{equation*}
Take expectations left and right, use~\cref{eq:74}, and independence of $N_\lambda$ and $N_\mu$. Near the end of the computation, use~\cref{eq:71}.
\end{hint}
\begin{solution}
\begin{align*}\label{eq:002}
\P{N_\lambda(t) + N_\mu(t) = n} 
&= \sum_{i=0}^n \P{ N_\mu(t) = n-i}\P{N_\lambda(t)=i} \\
&= \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} e^{-(\mu+\lambda)t} \\
&= e^{-(\mu+\lambda)t} \sum_{i=0}^n \frac{(\mu t)^{n-i}}{(n-i)!} \frac{(\lambda t)^i}{i!} \\
&= e^{-(\mu+\lambda)t}\frac 1{n!} \sum_{i=0}^n {n \choose i }(\mu t)^{n-i}(\lambda t)^i \quad\text{ (binomial formula)} \\
&= \frac{((\mu+\lambda)t)^n}{n!}e^{-(\mu+\lambda)t}.
 \end{align*}
\end{solution}
\end{exercise}

\begin{extra}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, use moment-generating functions to show that $N_\lambda + N_\mu$ is a Poisson process with rate $\lambda + \mu$.
\begin{hint}
 Use~\cref{eq:73} and~\cref{eq:75}.
\end{hint}

\begin{solution}
\begin{align*}
M_{N_\lambda(t)+N_\mu(t)}(s) 
&= M_{N_\lambda(t)}(s)\cdot M_{N_{\mu}(t)}(s) \\
&=\exp(\lambda t (e^s -1))\cdot \exp(\mu t(e^s-1)) \\
&= \exp((\lambda + \mu)t (e^s-1)).
\end{align*}
The last expression is the moment-generating function of a Poisson random variable with parameter $(\lambda+\mu)t$.
\end{solution}
\end{extra}



\begin{extra}
 If the Poisson arrival processes $N_\lambda$ and $N_\mu$ are independent, show that
 \begin{equation*}
 \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} =
\frac{\lambda}{\lambda+\mu}.
 \end{equation*}
 Note that the right-hand side does not depend on $h$, hence it
 holds for any time $h$, whether it is small or not. 
\begin{hint}
Use the standard formula for conditional probability and that $N_\lambda(t) + N_\mu(t) \sim \text{P}((\lambda + \mu)t)$. Interpret the result.
% Suppose
% we write $N(t)=N_\lambda(t) + N_\mu(t)$. Then
% \begin{equation*}
% \P{N_\lambda(h) = 1| N(h) = 1}
% \end{equation*}
% is the probability that $N_\lambda(h)=1$ and $N_\mu(h)=0$ \emph{given}
% that $N(t)=1$. Use the standard formula for conditional probability. 
\end{hint}
\begin{solution}
 With the above:
 \begin{align*}
& \P{N_\lambda(h) = 1 | N_\lambda(h) + N_\mu(h) = 1} \\
&= \frac{\P{N_\lambda(h) = 1, N_\lambda(h) + N_\mu(h) = 1} }{ \P{N_\lambda(h) + N_\mu(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1, N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h) = 1}} \\ 
&= \frac{\P{N_\lambda(h) = 1}\P{N_\mu(h) = 0}}{ \P{N_{\lambda+\mu}(h) = 1}} \\ 
&= \frac{\lambda h \exp(-\lambda h) \exp(-\mu h)}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda h \exp{(-(\lambda + \mu)h)}}{((\lambda+\mu)h)\exp{(-(\lambda+\mu)h)}}\\
&= \frac{\lambda}{\lambda+\mu}.
 \end{align*}

 Given that a customer arrived in $[0,t]$, the probability that it is of the first type is $\lambda/(\lambda+\mu)$. 
\end{solution}
\end{extra}


Besides merging Poisson streams, we can also consider the concept of \emph{splitting}, or \emph{thinning}, a stream into sub-streams, as follows.
% When a customer arrives, throw a coin that lands heads with probability $p$ and tails with $q=1-p$.
% When the coin lands heads, the customer is of type 1 (e.g., a woman), otherwise of type 2 (e.g., a child).
Model the stream of people passing by a shop as a Poisson process $N_\lambda$. In the figure below these arrivals are marked as crosses at the upper line. 
With probability $p$ a person decides, independent of anything else, to enter the shop; the crosses at the lower line are the customers that enter the shop.
In the figure, the Bernoulli random variable $B_1=1$ so that the first passerby enters the shop; the second passerby does not enter as $B_2=0$, and so on.

 \begin{center}
\begin{tikzpicture}[scale=1]
%\draw[[-{Triangle[open]},dotted] (0,10)--(8.5,10);
\draw[->] (0,2)--(10,2);
\node[left] at (0,2) {$N_\lambda$};
%\draw[->] (0,1)--(10,1);
%\node[left] at (0,1) {$B_k$};
\draw[->] (0,0)--(10,0);
\node[left] at (0,0) {$N_{\lambda p}$};

\draw[{Rays[]}-{Rays[]},dotted] (1,2.06)--(1,-0.06) 
node[below] {$B_1=1$};

\draw[{Rays[]}-{Circle[open]},dotted] (2.5,2.06)--(2.5,1.3) 
node[below] {$B_2=0$};

\draw[{Rays[]}-{Circle[open]},dotted] (4,2.06)--(4,1.3) 
node[below, fill=white] {$B_3=0$};

\draw[{Rays[]}-{Rays[]},dotted] (5,2.06)--(5,-0.06) 
node[below] {$B_4=1$};

\draw[{Rays[]}-{Rays[]},dotted] (6.5,2.06)--(6.5,-0.06) 
node[below] {$B_5=1$};


\draw[{Rays[]}-{Circle[open]},dotted] (7.5,2.06)--(7.5,1.3) 
node[below, fill=white] {$B_6=0$};

\end{tikzpicture}
 \end{center}


\begin{extra}
 Show with conditioning that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
\begin{hint}
Suppose that $N_1$ is the thinned stream, and $N$ the original stream. Condition on the total number of arrivals $N(t)=n$ up to time
 $t$. Then, realize that the probability that a person is of type 1 is $p$. Hence when you consider $n$ people in
 total, the number $N_1(t)$ of type 1 people is binomially distributed. Thus, given that $n$ people arrived, the probability of $k$ `successes' (i.e., arrivals of type 1), is 
 \begin{equation*}
 \P{N_1(t)=k \given N(t) = n} = {n \choose k} p^k (1-p)^{n-k}.
 \end{equation*}
Use~\cref{eq:70} to decompose the $\{N_1=k\}$, and~\cref{eq:76} at the end. 
\end{hint}
\begin{solution}
\begin{align*}
 \P{N_1(t) = k}
&= \sum_{n=k}^\infty \P{N_1(t) =k, N(t) = n} 
= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}\P{N(t)=n} \\
&= \sum_{n=k}^\infty \P{N_1(t) =k\given N(t) = n}e^{-\lambda t} \frac{(\lambda t)^n}{n!}\\
&= \sum_{n=k}^\infty {n \choose k} p^k (1-p)^{n-k} e^{-\lambda t} \frac{(\lambda t)^n}{n!}, \quad\text{by the hint}\\
&= e^{-\lambda t}\sum_{n=k}^\infty \frac{p^k (1-p)^{n-k}}{k! (n-k)!} (\lambda t)^n
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=k}^\infty \frac{(\lambda t (1-p))^{n-k}}{(n-k)!}\\
&= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} \sum_{n=0}^\infty \frac{(\lambda t (1-p))^{n}}{n!}
= e^{-\lambda t} \frac{(\lambda t p)^k}{k!} e^{\lambda t(1-p)} \\
&= e^{-\lambda t p} \frac{(\lambda t p)^k}{k!}.
\end{align*}
\end{solution}
\end{extra} 


\begin{exercise}\clabel{ex:1}
 Show with moment-generating functions that thinning the Poisson process $N_\lambda$ by means of Bernoulli random variables with success probability $p$ results in a Poisson process $N_{\lambda p}$.
\begin{hint}
Dropping the dependence of $N$ on $t$ for the moment for notational convenience, consider the random variable 
 \begin{equation*}
 Y = \sum_{i=1}^N Z_i,
 \end{equation*}
 with $N\sim P(\lambda)$ and $Z_i\sim B(p)$. Show that the moment-generating function of $Y$ is equal to the moment-generating
 function of a Poisson random variable with parameter $\lambda p$.
\end{hint}
\begin{solution}
Consider $Y=\sum_{i=1}^N Z_i$. Suppose that $N=n$, so that $n$
arrivals occurred. Then we throw $n$ independent coins with success probability
$p$. It is clear that $Y$ is indeed a thinned Poisson random variable.

Model the coins as a generic Bernoulli distributed random variable
$Z$. We first need
\begin{equation*}
 \E{e^{s Z}} = e^0 \P{Z=0} + e^{s} \P{Z=1} = (1-p) + e^s p.
\end{equation*}
Suppose that $N=n$, then since the outcomes $Z_i$ of the coins are i.i.d.,
\begin{equation*}
\E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n,
\end{equation*}
where we use~\cref{eq:73}. 

With ~\cref{eq:77}, 
\begin{align*}
 \E{e^{s Y}}
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^N Z_i} \1{N=n}} \\
&= \E{\sum_{n=0}^\infty e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i} \1{N=n}} \\
&= \sum_{n=0}^\infty \E{e^{s\sum_{i=1}^n Z_i}} \E{\1{N=n}}, \text{ by independence of $Z_i$ and $N$}, \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n \P{N=n} \\
&= \sum_{n=0}^\infty \left(1+p(e^s-1)\right)^n e^{-\lambda} \frac{\lambda^n}{n!}
= e^{-\lambda} \sum_{n=0}^\infty \frac{\left(1+p(e^s-1)\right)^n \lambda^n}{n!}\\
&= e^{-\lambda} \exp(\lambda (1+p(e^s-1))) = \exp(\lambda p (e^s - 1)).
\end{align*}
% Thus, $Y$ has the same moment-generating function as a Poisson
% distributed random variable with parameter $\lambda p$. Since
% the moment-generating function specifies the distribution uniquely,
% $Y\sim P(\lambda p)$.
\end{solution}
\end{exercise} 

The concepts of merging and thinning are useful to analyze queueing networks.
Suppose the departure stream of a machine splits into two sub-streams, e.g., a fraction $p$ of the jobs moves on to another machine and the rest ($1-p$) of the jobs leaves the system.
Then we can model the arrival stream at the second machine as a thinned stream (with probability $p$) of the departures of the first machine.
Merging occurs where the output streams of various stations arrive at another station.

\begin{exercise} \clabel{ex:96}
 Use moment-generating functions to prove that $N_n(t)$ converges to $N$, that is, the right-hand side in~\cref{eq:bin} converges to the Poisson distribution~\cref{eq:pois} when $n\to \infty, p\to 0$ such that $p n=\lambda t$ remains constant.
\begin{hint}
Solve~\cref{ex:53} and \cref{ex:1} first. Perhaps~\cref{ex:31} is also useful.
\end{hint}
\begin{solution}
Take $Y=\sum_{i=1}^n Z_i$ with $Z_i\sim B(p)$. Then, 
\begin{equation*}
M_Y(s) = \E{e^{s\sum_{i=1}^n Z_i}} = \left(\E{e^{s Z}}\right)^n = \left(1 + p (e^s - 1)\right)^n. 
\end{equation*}
Recall that $p= \lambda t/ n$. Then, with~\cref{eq:65},
\begin{equation*}
\lim_{n\to\infty} \left(1 + \frac{\lambda t}{n} (e^s - 1)\right)^n = \exp({\lambda t (e^s-1)}). 
\end{equation*}
Thus, the limit becomes equal to the moment-generating function of the Poisson distribution. 
\end{solution}

\end{exercise}


\opt{solutionfiles}{
\Closesolutionfile{hint}
\Closesolutionfile{ans}
\subsection*{Hints}
\input{hint}
\subsection*{Solutions}
\input{ans}
}


%\clearpage

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "../companion"
%%% End:
